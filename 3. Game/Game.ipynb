{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadi Babalou - 810199380"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Intelligence - CA#03: *Game* - Spring 2023 \\\n",
    "In this notebook, we will simulate othello game. \\\n",
    "This game is played by two agents. One of them uses minimax algorithm with alpha-beta pruning and the other one plays randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import turtle\n",
    "import copy\n",
    "import math\n",
    "\n",
    "nodes_expanded = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GUI\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```OthelloUI``` class is responsible for the GUI of the game. this class uses ```turtle``` library to draw the game board and the pieces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OthelloUI:\n",
    "    def __init__(self, board_size=6, square_size=60):\n",
    "        self.board_size = board_size\n",
    "        self.square_size = square_size\n",
    "        self.screen = turtle.Screen()\n",
    "        self.screen.setup(self.board_size * self.square_size + 50, self.board_size * self.square_size + 50)\n",
    "        self.screen.bgcolor('white')\n",
    "        self.screen.title('Othello')\n",
    "        self.pen = turtle.Turtle()\n",
    "        self.pen.hideturtle()\n",
    "        self.pen.speed(0)\n",
    "        turtle.tracer(0, 0)\n",
    "\n",
    "    def draw_board(self, board):\n",
    "        self.pen.penup()\n",
    "        x, y = -self.board_size / 2 * self.square_size, self.board_size / 2 * self.square_size\n",
    "        for i in range(self.board_size):\n",
    "            self.pen.penup()\n",
    "            for j in range(self.board_size):\n",
    "                self.pen.goto(x + j * self.square_size, y - i * self.square_size)\n",
    "                self.pen.pendown()\n",
    "                self.pen.fillcolor('green')\n",
    "                self.pen.begin_fill()\n",
    "                self.pen.setheading(0)\n",
    "                for _ in range(4):\n",
    "                    self.pen.forward(self.square_size)\n",
    "                    self.pen.right(90)\n",
    "                self.pen.penup()\n",
    "                self.pen.end_fill()\n",
    "                self.pen.goto(x + j * self.square_size + self.square_size / 2,\n",
    "                              y - i * self.square_size - self.square_size + 5)\n",
    "                if board[i][j] == 1:\n",
    "                    self.pen.fillcolor('white')\n",
    "                    self.pen.begin_fill()\n",
    "                    self.pen.circle(self.square_size / 2 - 5)\n",
    "                    self.pen.end_fill()\n",
    "                elif board[i][j] == -1:\n",
    "                    self.pen.fillcolor('black')\n",
    "                    self.pen.begin_fill()\n",
    "                    self.pen.circle(self.square_size / 2 - 5)\n",
    "                    self.pen.end_fill()\n",
    "\n",
    "        turtle.update()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Othello``` class is our main class. It is responsible for the game logic and the game flow. \\\n",
    "The ```Othello``` class has following attributes:\n",
    "* ```size```: size of the board\n",
    "* ```ui```: instance of ```OthelloUI``` class or ```None``` if we don't want to use GUI.\n",
    "* ```board```: the board itself as a 2D array of {-1, 0, 1}. -1 means black, 0 means empty, and 1 means white.\n",
    "* ```current_turn```: the current turn. -1 means black, and 1 means white.\n",
    "* ```minimax_depth```: the maximum depth of the minimax algorithm.\n",
    "* ```prune```: whether to use alpha-beta pruning or not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Othello``` class has following methods:\n",
    "* ```restart_game```: restarts the game.\n",
    "* ```get_winner```: returns the winner of the game at the current state.\n",
    "* ```get_valid_moves```: gets player turn and returns a list of valid moves for that player.\n",
    "* ```make_move```: gets player turn and a move - a tuple of (row, column) - and makes the move.\n",
    "* ```get_cpu_move```: returns a random move from the list of valid moves for the random agent.\n",
    "* ```get_human_move```: uses ```minimax``` method to get the best move for the human agent.\n",
    "* ```minimax```: gets player turn, current depth, alpha, and beta and recursively calculates the best move for that player. \n",
    "* ```get_score```: This is our evaluation function. It calculates the score of the board for the human agent and uses it as the heuristic value for the minimax algorithm.\n",
    "* ```terminal_test```: checks if the game is over or not.\n",
    "* ```play```: starts the game. It uses ```get_human_move``` and ```get_cpu_move``` to get the moves and uses ```make_move``` to make the moves. It also uses ```ui``` to draw the board and the pieces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Othello:\n",
    "    def __init__(self, ui, minimax_depth=1, prune=True):\n",
    "        self.size = 6\n",
    "        self.ui = OthelloUI(self.size) if ui else None\n",
    "        self.board = [[0 for _ in range(self.size)] for _ in range(self.size)]\n",
    "        self.board[int(self.size / 2) - 1][int(self.size / 2) - 1] = self.board[int(self.size / 2)][\n",
    "            int(self.size / 2)] = 1\n",
    "        self.board[int(self.size / 2) - 1][int(self.size / 2)] = self.board[int(self.size / 2)][\n",
    "            int(self.size / 2) - 1] = -1\n",
    "        self.current_turn = random.choice([1, -1])\n",
    "        self.minimax_depth = minimax_depth\n",
    "        self.prune = prune\n",
    "\n",
    "    def restart_game(self):\n",
    "        self.board = [[0 for _ in range(self.size)] for _ in range(self.size)]\n",
    "        self.board[int(self.size / 2) - 1][int(self.size / 2) - 1] = self.board[int(self.size / 2)][\n",
    "            int(self.size / 2)] = 1\n",
    "        self.board[int(self.size / 2) - 1][int(self.size / 2)] = self.board[int(self.size / 2)][\n",
    "            int(self.size / 2) - 1] = -1\n",
    "        self.current_turn = random.choice([1, -1])\n",
    "\n",
    "    def get_winner(self):\n",
    "        white_count = sum([row.count(1) for row in self.board])\n",
    "        black_count = sum([row.count(-1) for row in self.board])\n",
    "        if white_count > black_count:\n",
    "            return 1\n",
    "        elif white_count < black_count:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def get_valid_moves(self, player):\n",
    "        moves = set()\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if self.board[i][j] == 0:\n",
    "                    for di in [-1, 0, 1]:\n",
    "                        for dj in [-1, 0, 1]:\n",
    "                            if di == 0 and dj == 0:\n",
    "                                continue\n",
    "                            x, y = i, j\n",
    "                            captured = []\n",
    "                            while 0 <= x + di < self.size and 0 <= y + dj < self.size and self.board[x + di][\n",
    "                                    y + dj] == -player:\n",
    "                                captured.append((x + di, y + dj))\n",
    "                                x += di\n",
    "                                y += dj\n",
    "                            if 0 <= x + di < self.size and 0 <= y + dj < self.size and self.board[x + di][\n",
    "                                    y + dj] == player and len(captured) > 0:\n",
    "                                moves.add((i, j))\n",
    "        return list(moves)\n",
    "\n",
    "    def make_move(self, player, move):\n",
    "        i, j = move\n",
    "        self.board[i][j] = player\n",
    "        for di in [-1, 0, 1]:\n",
    "            for dj in [-1, 0, 1]:\n",
    "                if di == 0 and dj == 0:\n",
    "                    continue\n",
    "                x, y = i, j\n",
    "                captured = []\n",
    "                while 0 <= x + di < self.size and 0 <= y + dj < self.size and self.board[x + di][y + dj] == -player:\n",
    "                    captured.append((x + di, y + dj))\n",
    "                    x += di\n",
    "                    y += dj\n",
    "                if 0 <= x + di < self.size and 0 <= y + dj < self.size and self.board[x + di][y + dj] == player:\n",
    "                    for (cx, cy) in captured:\n",
    "                        self.board[cx][cy] = player\n",
    "\n",
    "    def get_cpu_move(self):\n",
    "        moves = self.get_valid_moves(-1)\n",
    "        if len(moves) == 0:\n",
    "            return None\n",
    "        return random.choice(moves)\n",
    "\n",
    "    def get_human_move(self):\n",
    "        move, score, _ = self.minimax(self.minimax_depth, 1, -math.inf, math.inf)\n",
    "        return move\n",
    "        \n",
    "    def minimax(self, depth, player, alpha, beta):\n",
    "        global nodes_expanded\n",
    "        nodes_expanded += 1\n",
    "        if depth == 0 or self.terminal_test():\n",
    "            return None, self.get_score(player, self.board), depth\n",
    "        \n",
    "        board_copy = copy.deepcopy(self.board)\n",
    "        best_move = None\n",
    "        best_score_depth = -1\n",
    "\n",
    "        if player == 1:\n",
    "            max_score = -math.inf\n",
    "            for move in self.get_valid_moves(player):\n",
    "                self.make_move(player, move)\n",
    "                _, score, rec_depth = self.minimax(depth - 1, -player, alpha, beta)\n",
    "\n",
    "                if score > max_score:\n",
    "                    max_score = score\n",
    "                    best_move = move\n",
    "                    best_score_depth = rec_depth\n",
    "                    alpha = max(alpha, max_score)\n",
    "                    if self.prune and max_score >= beta:\n",
    "                        break\n",
    "\n",
    "                elif score == max_score:\n",
    "                    if rec_depth > best_score_depth:\n",
    "                        best_move = move\n",
    "                        best_score_depth = rec_depth\n",
    "\n",
    "                self.board = copy.deepcopy(board_copy)\n",
    "\n",
    "            return best_move, max_score, best_score_depth\n",
    "        \n",
    "\n",
    "        elif player == -1:\n",
    "            min_score = math.inf\n",
    "            for move in self.get_valid_moves(player):\n",
    "                self.make_move(player, move)\n",
    "                _, score, rec_depth = self.minimax(depth - 1, -player, alpha, beta)\n",
    "                \n",
    "                if score < min_score:\n",
    "                    min_score = score\n",
    "                    best_move = move\n",
    "                    best_score_depth = rec_depth\n",
    "                    beta = min(beta, min_score)\n",
    "                    if self.prune and min_score <= alpha:\n",
    "                        break\n",
    "\n",
    "                elif score == min_score:\n",
    "                    if rec_depth > best_score_depth:\n",
    "                        best_move = move\n",
    "                        best_score_depth = rec_depth\n",
    "\n",
    "                self.board = copy.deepcopy(board_copy)\n",
    "\n",
    "            return best_move, min_score, best_score_depth    \n",
    "    \n",
    "    def get_score(self, player, board):\n",
    "        score = 0\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                score += board[i][j]\n",
    "        return score\n",
    "\n",
    "    def set_minimax_depth(self, depth):\n",
    "        self.minimax_depth = depth\n",
    "\n",
    "    def set_prune(self, prune):\n",
    "        self.prune = prune\n",
    "\n",
    "    def terminal_test(self):\n",
    "        return len(self.get_valid_moves(1)) == 0 and len(self.get_valid_moves(-1)) == 0\n",
    "\n",
    "    def play(self):\n",
    "        winner = None\n",
    "        round_num = 1\n",
    "        global nodes_expanded\n",
    "        nodes_expanded = 0\n",
    "        while not self.terminal_test():\n",
    "            if round_num > 40:\n",
    "                self.restart_game()\n",
    "                return 1\n",
    "            if self.current_turn == 1:\n",
    "                move = self.get_human_move()\n",
    "                if move:\n",
    "                    self.make_move(self.current_turn, move)\n",
    "            else:\n",
    "                move = self.get_cpu_move()\n",
    "                if move:\n",
    "                    self.make_move(self.current_turn, move)\n",
    "            self.current_turn = -self.current_turn\n",
    "            if self.ui:\n",
    "                self.ui.draw_board(self.board)\n",
    "                time.sleep(0.25)\n",
    "            round_num += 1\n",
    "        winner = self.get_winner()\n",
    "        self.restart_game()\n",
    "        return winner\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic Function\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our heuristic function is very simple and it's implemented in the ```get_score``` method of the ```Othello``` class. \\\n",
    "The score is calculated by the following formula: \n",
    "$$ \\text{score} = \\text{number of white pieces} - \\text{number of black pieces} $$\n",
    "This function is very simple and it doesn't consider the position of the pieces, but because the other agent plays randomly, this function is enough to win the game. \\\n",
    "To improve the performance of the algorithm, we can use a more complex heuristic function that considers the position of the pieces. We can use the following formula for example: \\\n",
    "    $$ \\text{score} = \\text{number of white pieces} - \\text{number of black pieces} + w_{corner} * (\\text{number of white pieces in the corners} - \\text{number of black pieces in the corners}) + w_{edge} * (\\text{number of white pieces in the edges} - \\text{number of black pieces in the edges}) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation and Results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we play 1 game with GUI and depth of 3 to see how the game works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner of game: 1, nodes expanded: 1660, time: 9.279 seconds"
     ]
    }
   ],
   "source": [
    "MINIMAX_DEPTH = 3\n",
    "UI = True\n",
    "PRUNE = True\n",
    "\n",
    "othello = Othello(ui=UI, minimax_depth=MINIMAX_DEPTH, prune=PRUNE)\n",
    "start = time.time()\n",
    "winner = othello.play()\n",
    "print(f'Winner of game: {winner}, nodes expanded: {nodes_expanded}, time: {time.time() - start:.3f} seconds', end=\"\")\n",
    "end = time.time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we play multiple games with different depths without GUI and prunning to see how the performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINIMAX_DEPTH: 1                                                                                    \n",
      "Time taken: 2.180 seconds\n",
      "Average time taken per game: 0.015 seconds\n",
      "White wins: 86\n",
      "Black wins: 57\n",
      "Draws: 7\n",
      "White win percentage: 60.14%\n",
      "Nodes expanded mean: 94.43\n"
     ]
    }
   ],
   "source": [
    "MINIMAX_DEPTH = 1\n",
    "UI = False\n",
    "PRUNE = False\n",
    "REPEAT = 150\n",
    "\n",
    "othello = Othello(ui=UI, minimax_depth=MINIMAX_DEPTH, prune=PRUNE)\n",
    "winners = [0, 0, 0]\n",
    "start = time.time()\n",
    "nodes_expanded_sum = 0\n",
    "for i in range(REPEAT):\n",
    "    winner = othello.play()\n",
    "    print('\\r', end=\"\")\n",
    "    print(100 * ' ', end=\"\")\n",
    "    print('\\r', end=\"\")\n",
    "    print(f'Winner of game {i+1}: {winner}, nodes expanded: {nodes_expanded}, time: {time.time() - start:.3f} seconds', end=\"\")\n",
    "    winners[winner + 1] += 1\n",
    "    nodes_expanded_sum += nodes_expanded\n",
    "end = time.time()\n",
    "\n",
    "print('\\r', end=\"\")\n",
    "print(100 * ' ', end=\"\")\n",
    "print('\\r', end=\"\")\n",
    "print(f'MINIMAX_DEPTH: {MINIMAX_DEPTH}')\n",
    "print(f'Time taken: {end - start:.3f} seconds')\n",
    "print(f'Average time taken per game: {(end - start) / REPEAT:.3f} seconds')\n",
    "print(f'White wins: {winners[2]}')\n",
    "print(f'Black wins: {winners[0]}')\n",
    "print(f'Draws: {winners[1]}')\n",
    "print(f'White win percentage: {winners[2] / (winners[0] + winners[2]) * 100:.2f}%')\n",
    "print(f'Nodes expanded mean: {nodes_expanded_sum / REPEAT:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINIMAX_DEPTH: 3                                                                                    \n",
      "Time taken: 33.711 seconds\n",
      "Average time taken per game: 0.225 seconds\n",
      "White wins: 119\n",
      "Black wins: 25\n",
      "Draws: 6\n",
      "White win percentage: 82.64%\n",
      "Nodes expanded mean: 3122.71\n"
     ]
    }
   ],
   "source": [
    "MINIMAX_DEPTH = 3\n",
    "UI = False\n",
    "PRUNE = False\n",
    "REPEAT = 150\n",
    "\n",
    "othello = Othello(ui=UI, minimax_depth=MINIMAX_DEPTH, prune=PRUNE)\n",
    "winners = [0, 0, 0]\n",
    "start = time.time()\n",
    "nodes_expanded_sum = 0\n",
    "for i in range(REPEAT):\n",
    "    winner = othello.play()\n",
    "    print('\\r', end=\"\")\n",
    "    print(100 * ' ', end=\"\")\n",
    "    print('\\r', end=\"\")\n",
    "    print(f'Winner of game {i+1}: {winner}, nodes expanded: {nodes_expanded}, time: {time.time() - start:.3f} seconds', end=\"\")\n",
    "    winners[winner + 1] += 1\n",
    "    nodes_expanded_sum += nodes_expanded\n",
    "end = time.time()\n",
    "\n",
    "print('\\r', end=\"\")\n",
    "print(100 * ' ', end=\"\")\n",
    "print('\\r', end=\"\")\n",
    "print(f'MINIMAX_DEPTH: {MINIMAX_DEPTH}')\n",
    "print(f'Time taken: {end - start:.3f} seconds')\n",
    "print(f'Average time taken per game: {(end - start) / REPEAT:.3f} seconds')\n",
    "print(f'White wins: {winners[2]}')\n",
    "print(f'Black wins: {winners[0]}')\n",
    "print(f'Draws: {winners[1]}')\n",
    "print(f'White win percentage: {winners[2] / (winners[0] + winners[2]) * 100:.2f}%')\n",
    "print(f'Nodes expanded mean: {nodes_expanded_sum / REPEAT:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINIMAX_DEPTH: 5                                                                                    \n",
      "Time taken: 795.892 seconds\n",
      "Average time taken per game: 7.959 seconds\n",
      "White wins: 96\n",
      "Black wins: 3\n",
      "Draws: 1\n",
      "White win percentage: 96.97%\n",
      "Nodes expanded mean: 111867.57\n"
     ]
    }
   ],
   "source": [
    "MINIMAX_DEPTH = 5\n",
    "UI = False\n",
    "PRUNE = False\n",
    "REPEAT = 100\n",
    "\n",
    "othello = Othello(ui=UI, minimax_depth=MINIMAX_DEPTH, prune=PRUNE)\n",
    "winners = [0, 0, 0]\n",
    "start = time.time()\n",
    "nodes_expanded_sum = 0\n",
    "for i in range(REPEAT):\n",
    "    winner = othello.play()\n",
    "    print('\\r', end=\"\")\n",
    "    print(100 * ' ', end=\"\")\n",
    "    print('\\r', end=\"\")\n",
    "    print(f'Winner of game {i+1}: {winner}, nodes expanded: {nodes_expanded}, time: {time.time() - start:.3f} seconds', end=\"\")\n",
    "    winners[winner + 1] += 1\n",
    "    nodes_expanded_sum += nodes_expanded\n",
    "end = time.time()\n",
    "\n",
    "print('\\r', end=\"\")\n",
    "print(100 * ' ', end=\"\")\n",
    "print('\\r', end=\"\")\n",
    "print(f'MINIMAX_DEPTH: {MINIMAX_DEPTH}')\n",
    "print(f'Time taken: {end - start:.3f} seconds')\n",
    "print(f'Average time taken per game: {(end - start) / REPEAT:.3f} seconds')\n",
    "print(f'White wins: {winners[2]}')\n",
    "print(f'Black wins: {winners[0]}')\n",
    "print(f'Draws: {winners[1]}')\n",
    "print(f'White win percentage: {winners[2] / (winners[0] + winners[2]) * 100:.2f}%')\n",
    "print(f'Nodes expanded mean: {nodes_expanded_sum / REPEAT:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we enable prunning to see how it affects the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINIMAX_DEPTH: 1                                                                                    \n",
      "Time taken: 1.584 seconds\n",
      "Average time taken per game: 0.011 seconds\n",
      "White wins: 83\n",
      "Black wins: 57\n",
      "Draws: 10\n",
      "White win percentage: 59.29%\n",
      "Nodes expanded mean: 94.87\n"
     ]
    }
   ],
   "source": [
    "MINIMAX_DEPTH = 1\n",
    "UI = False\n",
    "PRUNE = True\n",
    "REPEAT = 150\n",
    "\n",
    "othello = Othello(ui=UI, minimax_depth=MINIMAX_DEPTH, prune=PRUNE)\n",
    "winners = [0, 0, 0]\n",
    "start = time.time()\n",
    "nodes_expanded_sum = 0\n",
    "for i in range(REPEAT):\n",
    "    winner = othello.play()\n",
    "    print('\\r', end=\"\")\n",
    "    print(100 * ' ', end=\"\")\n",
    "    print('\\r', end=\"\")\n",
    "    print(f'Winner of game {i+1}: {winner}, nodes expanded: {nodes_expanded}, time: {time.time() - start:.3f} seconds', end=\"\")\n",
    "    winners[winner + 1] += 1\n",
    "    nodes_expanded_sum += nodes_expanded\n",
    "end = time.time()\n",
    "\n",
    "print('\\r', end=\"\")\n",
    "print(100 * ' ', end=\"\")\n",
    "print('\\r', end=\"\")\n",
    "print(f'MINIMAX_DEPTH: {MINIMAX_DEPTH}')\n",
    "print(f'Time taken: {end - start:.3f} seconds')\n",
    "print(f'Average time taken per game: {(end - start) / REPEAT:.3f} seconds')\n",
    "print(f'White wins: {winners[2]}')\n",
    "print(f'Black wins: {winners[0]}')\n",
    "print(f'Draws: {winners[1]}')\n",
    "print(f'White win percentage: {winners[2] / (winners[0] + winners[2]) * 100:.2f}%')\n",
    "print(f'Nodes expanded mean: {nodes_expanded_sum / REPEAT:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINIMAX_DEPTH: 3                                                                                    \n",
      "Time taken: 14.748 seconds\n",
      "Average time taken per game: 0.098 seconds\n",
      "White wins: 107\n",
      "Black wins: 37\n",
      "Draws: 6\n",
      "White win percentage: 74.31%\n",
      "Nodes expanded mean: 1245.87\n"
     ]
    }
   ],
   "source": [
    "MINIMAX_DEPTH = 3\n",
    "UI = False\n",
    "PRUNE = True\n",
    "REPEAT = 150\n",
    "\n",
    "othello = Othello(ui=UI, minimax_depth=MINIMAX_DEPTH, prune=PRUNE)\n",
    "winners = [0, 0, 0]\n",
    "start = time.time()\n",
    "nodes_expanded_sum = 0\n",
    "for i in range(REPEAT):\n",
    "    winner = othello.play()\n",
    "    print('\\r', end=\"\")\n",
    "    print(100 * ' ', end=\"\")\n",
    "    print('\\r', end=\"\")\n",
    "    print(f'Winner of game {i+1}: {winner}, nodes expanded: {nodes_expanded}, time: {time.time() - start:.3f} seconds', end=\"\")\n",
    "    winners[winner + 1] += 1\n",
    "    nodes_expanded_sum += nodes_expanded\n",
    "end = time.time()\n",
    "\n",
    "print('\\r', end=\"\")\n",
    "print(100 * ' ', end=\"\")\n",
    "print('\\r', end=\"\")\n",
    "print(f'MINIMAX_DEPTH: {MINIMAX_DEPTH}')\n",
    "print(f'Time taken: {end - start:.3f} seconds')\n",
    "print(f'Average time taken per game: {(end - start) / REPEAT:.3f} seconds')\n",
    "print(f'White wins: {winners[2]}')\n",
    "print(f'Black wins: {winners[0]}')\n",
    "print(f'Draws: {winners[1]}')\n",
    "print(f'White win percentage: {winners[2] / (winners[0] + winners[2]) * 100:.2f}%')\n",
    "print(f'Nodes expanded mean: {nodes_expanded_sum / REPEAT:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINIMAX_DEPTH: 5                                                                                    \n",
      "Time taken: 120.601 seconds\n",
      "Average time taken per game: 1.206 seconds\n",
      "White wins: 94\n",
      "Black wins: 4\n",
      "Draws: 2\n",
      "White win percentage: 95.92%\n",
      "Nodes expanded mean: 13604.04\n"
     ]
    }
   ],
   "source": [
    "MINIMAX_DEPTH = 5\n",
    "UI = False\n",
    "PRUNE = True\n",
    "REPEAT = 100\n",
    "\n",
    "othello = Othello(ui=UI, minimax_depth=MINIMAX_DEPTH, prune=PRUNE)\n",
    "winners = [0, 0, 0]\n",
    "start = time.time()\n",
    "nodes_expanded_sum = 0\n",
    "for i in range(REPEAT):\n",
    "    winner = othello.play()\n",
    "    print('\\r', end=\"\")\n",
    "    print(100 * ' ', end=\"\")\n",
    "    print('\\r', end=\"\")\n",
    "    print(f'Winner of game {i+1}: {winner}, nodes expanded: {nodes_expanded}, time: {time.time() - start:.3f} seconds', end=\"\")\n",
    "    winners[winner + 1] += 1\n",
    "    nodes_expanded_sum += nodes_expanded\n",
    "end = time.time()\n",
    "\n",
    "print('\\r', end=\"\")\n",
    "print(100 * ' ', end=\"\")\n",
    "print('\\r', end=\"\")\n",
    "print(f'MINIMAX_DEPTH: {MINIMAX_DEPTH}')\n",
    "print(f'Time taken: {end - start:.3f} seconds')\n",
    "print(f'Average time taken per game: {(end - start) / REPEAT:.3f} seconds')\n",
    "print(f'White wins: {winners[2]}')\n",
    "print(f'Black wins: {winners[0]}')\n",
    "print(f'Draws: {winners[1]}')\n",
    "print(f'White win percentage: {winners[2] / (winners[0] + winners[2]) * 100:.2f}%')\n",
    "print(f'Nodes expanded mean: {nodes_expanded_sum / REPEAT:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINIMAX_DEPTH: 7                                                                                    \n",
      "Time taken: 1285.036 seconds\n",
      "Average time taken per game: 12.850 seconds\n",
      "White wins: 95\n",
      "Black wins: 5\n",
      "Draws: 0\n",
      "White win percentage: 95.00%\n",
      "Nodes expanded mean: 136185.31\n"
     ]
    }
   ],
   "source": [
    "MINIMAX_DEPTH = 7\n",
    "UI = False\n",
    "PRUNE = True\n",
    "REPEAT = 100\n",
    "\n",
    "othello = Othello(ui=UI, minimax_depth=MINIMAX_DEPTH, prune=PRUNE)\n",
    "winners = [0, 0, 0]\n",
    "start = time.time()\n",
    "nodes_expanded_sum = 0\n",
    "for i in range(REPEAT):\n",
    "    winner = othello.play()\n",
    "    print('\\r', end=\"\")\n",
    "    print(100 * ' ', end=\"\")\n",
    "    print('\\r', end=\"\")\n",
    "    print(f'Winner of game {i+1}: {winner}, nodes expanded: {nodes_expanded}, time: {time.time() - start:.3f} seconds', end=\"\")\n",
    "    winners[winner + 1] += 1\n",
    "    nodes_expanded_sum += nodes_expanded\n",
    "end = time.time()\n",
    "\n",
    "print('\\r', end=\"\")\n",
    "print(100 * ' ', end=\"\")\n",
    "print('\\r', end=\"\")\n",
    "print(f'MINIMAX_DEPTH: {MINIMAX_DEPTH}')\n",
    "print(f'Time taken: {end - start:.3f} seconds')\n",
    "print(f'Average time taken per game: {(end - start) / REPEAT:.3f} seconds')\n",
    "print(f'White wins: {winners[2]}')\n",
    "print(f'Black wins: {winners[0]}')\n",
    "print(f'Draws: {winners[1]}')\n",
    "print(f'White win percentage: {winners[2] / (winners[0] + winners[2]) * 100:.2f}%')\n",
    "print(f'Nodes expanded mean: {nodes_expanded_sum / REPEAT:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How did you design your heuristic function and why?\n",
    "A good heuristic function should be fast to calculate and it should be able to estimate the score of the board for a player. \\\n",
    "details of the heuristic function is explained in the heuristic function section. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What are the effects of the depth of the search tree on the performance of the algorithm?\n",
    "As we can see in the results, by increasing the depth of the search tree, the number of nodes that the algorithm has to visit increases and therefore the time that the algorithm needs to find the best move increases. \\\n",
    "We can see that the time and number of nodes increases exponentially with the depth of the search tree. \\\n",
    "But in exchange, we can win the game with a higher probability because the algorithm can search deeper in the search tree and find better moves.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Can we change the order of visiting the children of a node in the minimax algorithm so that we can prune more branches? If so, how?\n",
    "In alpha-beta pruning, the order of visiting child nodes is crucial for maximizing the number of nodes pruned. The basic idea is to visit the most promising nodes first, i.e., those that are most likely to lead to a successful outcome. \\\n",
    "One common heuristic is to order the child nodes based on their evaluation function values. In particular, the best child node is visited first, followed by the second-best, and so on. This is known as the \"static ordering\" strategy, as the order of visiting the child nodes is fixed before the search begins. \\\n",
    "Another approach is to use a \"dynamic ordering\" strategy, which updates the order of child nodes during the search based on the current state of the game. For example, if a child node has been found to be promising, it can be moved to the front of the order to be explored first. \\\n",
    "There are also more sophisticated methods that use machine learning or other techniques to learn the best ordering of child nodes. These methods can be highly effective, but they may require more computational resources and training data. \\\n",
    "Ultimately, the choice of ordering strategy will depend on the specific problem domain and the available computational resources. However, by carefully considering the order of visiting child nodes, it is possible to prune a maximum number of nodes and speed up the search process in alpha-beta pruning.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explain what is \"Branching Factor\" and how it changes during the process of the game?\n",
    "The branching factor is a measure of the number of possible moves that can be made at each level of a search tree. In the context of game playing algorithms like Minimax with Alpha-Beta Pruning, the branching factor refers to the number of legal moves available to the player at any given point in the game. \\\n",
    "When playing Othello using Minimax with Alpha-Beta Pruning, the branching factor will change over time as the game progresses. In the beginning, the branching factor will be relatively high, as there are many possible moves available to both players. However, as the game progresses and more pieces are placed on the board, the number of legal moves available will decrease, and the branching factor will decrease accordingly. \\\n",
    "In the early stages of the game, the branching factor can be quite high, often ranging from 20 to 30 moves. However, as the game progresses and the board becomes more crowded, the branching factor will decrease, sometimes to as few as 5 to 10 moves per turn. \\\n",
    "Minimax with Alpha-Beta Pruning is designed to take advantage of this changing branching factor by exploring the most promising moves first and pruning branches of the tree that are unlikely to lead to a successful outcome. By carefully choosing which branches to explore, this algorithm can effectively search the game tree and find a strong move while minimizing the number of nodes it needs to explore. This is particularly useful in games like Othello, where the branching factor can be quite large in the early game but decreases as the game progresses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Why pruning makes the algorithm faster without decreasing the quality of the solution?\n",
    "Pruning is a technique used in the minimax algorithm with alpha-beta pruning to reduce the number of nodes that need to be evaluated during the search process. It works by eliminating parts of the search tree that are known to be irrelevant to the final solution, thereby reducing the computational time required to find the optimal solution. \\\n",
    "The minimax algorithm with alpha-beta pruning is a depth-first search algorithm that evaluates every possible move from the current state of the game. During the search process, the algorithm maintains two values: alpha, which represents the best value found so far for the maximizing player, and beta, which represents the best value found so far for the minimizing player. These values are used to prune branches of the search tree that cannot lead to a better solution than what has already been found. \\\n",
    "The benefit of pruning is that it eliminates large portions of the search space that are guaranteed to be suboptimal. By reducing the number of nodes that need to be evaluated, pruning significantly reduces the computational time required to find the optimal solution. \\\n",
    "Importantly, pruning does not decrease the quality of the solution because it only eliminates nodes that are guaranteed to be suboptimal. The algorithm will still explore all of the nodes that are relevant to the final solution, ensuring that the optimal solution is found. \\\n",
    "Furthermore, the effectiveness of alpha-beta pruning increases as the depth of the search tree increases. This is because the number of nodes that need to be evaluated grows exponentially with the depth of the tree, making it increasingly important to eliminate irrelevant nodes.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Why when the other agent plays randomly, using the minimax isn't the optimal solution? which algorithm is better in this case?\n",
    "When the opposing agent plays randomly, using the minimax algorithm with alpha-beta pruning may not be the optimal solution. This is because the minimax algorithm assumes that the opponent is playing optimally, and therefore evaluates every possible move as if the opponent is trying to make the best move possible. However, if the opponent is playing randomly, the minimax algorithm may waste a lot of computational resources evaluating moves that are not optimal in practice. \\\n",
    "In this case, a more appropriate algorithm to use might be a Monte Carlo Tree Search (MCTS) algorithm. MCTS is a heuristic-based search algorithm that is particularly effective when the opponent is playing randomly or sub-optimally. \\\n",
    "MCTS works by building a tree of possible moves and then simulating random games from the current node to the end of the game. The algorithm uses the results of these simulations to guide the search towards the most promising moves. Unlike minimax, MCTS does not assume that the opponent is playing optimally and instead focuses on exploring the most promising branches of the search tree. \\\n",
    "MCTS can be a particularly effective algorithm when playing against opponents who are playing randomly or sub-optimally because it does not waste computational resources evaluating moves that are not likely to be played by the opponent. Instead, it focuses on exploring the most promising branches of the search tree based on the results of simulated games. \\\n",
    "In summary, when the opposing agent is playing randomly, using the minimax algorithm with alpha-beta pruning may not be the optimal solution. Instead, a Monte Carlo Tree Search algorithm may be more appropriate due to its ability to efficiently explore the most promising branches of the search tree based on the results of simulated games.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
